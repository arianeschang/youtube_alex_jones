{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Alex Jones Videos Data Collection \n",
    "\n",
    "On his YouTube channel, Alex Jones often posts videos attacking the credibility of mainstream media. We are interested in doing a small analysis of this phenonemon to see if we might be able to characterize these attacks. \n",
    "\n",
    "First, we'll need to collect the metadata for all the videos posted between January 1st, 2015 and May 4th, 2018 (as specified). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the pandas library because it's useful and quick when manipulating large amounts of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first need to write a function that accesses the YouTube API. We'll be able to use this everytime we need to query some information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query_type, parameters):\n",
    "    '''\n",
    "    inputs: query_type - the resource type that we need information for (e.g. playlists, channels, videos, etc.)\n",
    "            parameters - list of tuples correpsonding to the parameter names and their values in our query \n",
    "    '''\n",
    "    base_url = 'https://www.googleapis.com/youtube/v3/'\n",
    "    key = \"mykey\"\n",
    "    \n",
    "    # We always need to pass our key, so we'll add it to the list here\n",
    "    parameters = parameters.append(('key', key))\n",
    "    \n",
    "    # Takes a list of tuples and encodes it into a url like:\n",
    "    # 'part=contentDetails&key=mykey&forUsername=theAlexJonesChannel'\n",
    "    url_suffix = urllib.urlencode(parameters)\n",
    "    \n",
    "    # Constructs the url to query and returns the json response\n",
    "    http_endpoint = base_url + query_type + url_suffix\n",
    "    response = requests.get(http_endpoint)\n",
    "    response_json = response.json()\n",
    "    return response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a users' uploaded videos, we'll find the playlist called uploads and then go through the list to find the relevant information for each video. \n",
    "\n",
    "The YouTube API only returns a maximum of 50 results with every call, so we'll need to move through the pages to get the data using the nextPageToken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_videos(username):\n",
    "    '''\n",
    "    Make a pandas dataframe with all the videos uploaded by this user. \n",
    "    \n",
    "    Inputs: username - the user whose videos we want to list\n",
    "    '''\n",
    "    \n",
    "    video_data = []\n",
    "\n",
    "    # Find the id of the UPLOADS playlist, and the channel_id\n",
    "    playlist_params = [('part', 'contentDetails,id'), ('forUsername', username)]\n",
    "    playlist_info = get_response('channels?', playlist_params)\n",
    "    \n",
    "    playlist_ID = playlist_info['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    channel_Id = playlist_info['items'][0]['id']\n",
    "    \n",
    "    # Get videos in the playlist\n",
    "    video_params = [('part','snippet'), ('playlistId',playlist_ID), ('maxResults', 50)]\n",
    "    video_search = get_response('playlistItems?', video_params)\n",
    "    \n",
    "    \n",
    "    # The information we'll be collecting for each video\n",
    "    headers = ['video_id', 'channel_title', 'channel_id', 'video_publish_date', \n",
    "                'video_title', 'video_view_count', 'video_like_count', 'video_dislike_count',\n",
    "                'video_comment_count', 'video_description']\n",
    "\n",
    "    while True:\n",
    "        for video in video_search['items']:\n",
    "            video_snippet = video['snippet']\n",
    "\n",
    "            # The YouTube API does not let us filter the playlist search results by date\n",
    "            # so we'll check if the video falls in our desired time frame before \n",
    "            # processing it. \n",
    "            # Because the videos are largely in descending order, we can break and stop \n",
    "            #searching after we hit January 1st, 2015. \n",
    "            if video_snippet['publishedAt'] > '2018-05-05T00:00:00Z':\n",
    "                continue\n",
    "            elif video_snippet['publishedAt'] < '2015-01-01T00:00:00Z':\n",
    "                return video_data\n",
    "            \n",
    "            # Get the information we need and then add to our whole list of data. \n",
    "            this_video_data = get_video_data(video_snippet, channel_Id)\n",
    "            video_data.append(this_video_data)\n",
    "\n",
    "        # Get the next video. \n",
    "        next_video = video_params + [('pageToken', video_search['nextPageToken'])]\n",
    "        video_search = get_response('playlistItems?', next_video)\n",
    "\n",
    "        # Save our data so far just to be safe \n",
    "        video_df = pd.DataFrame(video_data, columns=headers)\n",
    "        video_df.to_pickle(\"data/video_data.pkl\")\n",
    "\n",
    "        # If there's no more results, we break and return our dataframe\n",
    "        if len(video_search['items']) == 0:\n",
    "            return video_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each video, we'll need to extract the information we want. To get the like, dislike, view and comment counts, we'll need to run one more query to the YouTube API, videos section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_data(video_snippet, channel_Id):\n",
    "    \n",
    "    '''\n",
    "    Takes a snippet of information about a video and turns it into a set of attributes\n",
    "    that we can add to our dataset. We query the YouTube API using the video ID to get the\n",
    "    relevant statistics for that video. \n",
    "    \n",
    "    Inputs: video_snippet - json object containing relevant information about the video\n",
    "            vhannel_Id - the channel ID of our user\n",
    "    '''\n",
    "\n",
    "    video_id = video_snippet['resourceId']['videoId']\n",
    "\n",
    "    video_channel = video_snippet['channelTitle']\n",
    "    video_date = video_snippet['publishedAt']\n",
    "    video_title = video_snippet['title']\n",
    "    video_description = video_snippet['description'].split('\\n')[0].strip()\n",
    "\n",
    "    # Another query to find the statistics that we need for the video\n",
    "    video_params = [('part', 'statistics'), ('id', video_id)]\n",
    "    video_json = get_response('videos?', video_params)\n",
    "\n",
    "    # Arrange all our data into a list and return it\n",
    "    stats = video_json['items'][0]['statistics']\n",
    "    \n",
    "    video_view_count = get_value(stats, 'viewCount')\n",
    "    video_like_count = get_value(stats, 'likeCount')\n",
    "    video_dislike_count = get_value(stats, 'dislikeCount')\n",
    "    video_comment_count = get_value(stats, 'commentCount')\n",
    "\n",
    "    this_video = [video_id, video_channel, channel_Id, video_date, video_title, video_view_count,\n",
    "            video_like_count, video_dislike_count, video_comment_count, video_description]\n",
    "    \n",
    "    return this_video\n",
    "\n",
    "def get_value(dictionary, key):\n",
    "    # Sometimes, keys don't exist so we write these as None in our data.  \n",
    "    if key in dictionary:\n",
    "        return dictionary[key]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    key = \"mykey\"\n",
    "    username = \"TheAlexJonesChannel\"\n",
    "\n",
    "    # If we haven't yet extracted the data\n",
    "    video_data = get_all_videos(key, username)\n",
    "\n",
    "    # If we have the dataset pickled\n",
    "    #video_data = pd.read_pickle(\"data/video_data.pkl\")\n",
    "    \n",
    "    # Write our data to a CSV\n",
    "    video_data.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
